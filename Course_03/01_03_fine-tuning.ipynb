{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "b4d2b822-4e09-43e4-9990-0dc2a80d85f5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install modules\n",
        "# A '!' in a Jupyter Notebook runs the line in the system's shell, and not in the Python interpreter\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load dataset \n",
        "# you can download this dataset from https://huggingface.co/datasets/stepp1/tweet_emotion_intensity/tree/main\n",
        "data = pd.read_csv('data/tweet_emotion_intensity/train.csv')\n",
        "\n",
        "# Preview the data\n",
        "print(data.head())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "      id                                              tweet    class  \\\n0  40815  Loved @Bethenny independence msg on @WendyWill...     fear   \n1  10128  @mark_slifer actually maybe we were supposed t...  sadness   \n2  40476  I thought the nausea and headaches had passed ...     fear   \n3  20813  Anger, resentment, and hatred are the destroye...    anger   \n4  40796  new tires &amp; an alarm system on my car. fwm...     fear   \n\n  sentiment_intensity class_intensity  labels  \n0                 low        fear_low       4  \n1                high    sadness_high       9  \n2              medium     fear_medium       5  \n3                high      anger_high       0  \n4                 low        fear_low       4  \n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1749677348527
        }
      },
      "id": "1c38d690"
    },
    {
      "cell_type": "code",
      "source": [
        "import re # Import the `re` module for working with regular expressions\n",
        "\n",
        "# Function to clean the text\n",
        "def clean_text(text):\n",
        "    text = text.lower() # Convert all text to lowercase for uniformity\n",
        "    text = re.sub(r'http\\S+', '', text) # Remove URLs from the text\n",
        "    text = re.sub(r'<.*?>', '', text) # Remove any HTML tags from the text\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation, keep only words and spaces\n",
        "    return text # Return the cleaned text\n",
        "\n",
        "# Assume `data` is a pandas DataFrame with a column named 'text'\n",
        "# Apply the cleaning function to each row of the 'text' column\n",
        "data['cleaned_text'] = data['tweet'].apply(clean_text)\n",
        "\n",
        "# Print the first 5 rows of the cleaned text to verify the cleaning process\n",
        "print(data['cleaned_text'].head())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0    loved bethenny independence msg on wendywillia...\n1    mark_slifer actually maybe we were supposed to...\n2    i thought the nausea and headaches had passed ...\n3    anger resentment and hatred are the destroyer ...\n4      new tires amp an alarm system on my car fwm now\nName: cleaned_text, dtype: object\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1749677348815
        }
      },
      "id": "26033b2c"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the dataset\n",
        "print(data.isnull().sum()) # Print the count of missing values for each column\n",
        "\n",
        "# Option 1: Remove rows with missing data in the 'cleaned_text' column\n",
        "data = data.dropna(subset=['cleaned_text']) # Drop rows where 'cleaned_text' is NaN (missing)\n",
        "\n",
        "# Option 2: Fill missing values in 'cleaned_text' with a placeholder\n",
        "data['cleaned_text'].fillna('unknown', inplace=True) # Replace NaN values in 'cleaned_text' with 'unknown'"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "id                     0\ntweet                  0\nclass                  0\nsentiment_intensity    0\nclass_intensity        0\nlabels                 0\ncleaned_text           0\ndtype: int64\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1749677349179
        }
      },
      "id": "5093335d"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the cleaned text\n",
        "tokens = tokenizer(\n",
        "    data['cleaned_text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt'\n",
        ")\n",
        "\n",
        "print(tokens['input_ids'][:5])  # Preview the first 5 tokenized examples"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tensor([[  101,  3866,  7014,  2368,  4890,  4336,  5796,  2290,  2006, 12815,\n         29602,  6632,  5244,  2022,  3407, 23713, 16829,  2306,  4426, 23713,\n         13433, 28032,  7730,  2097, 19311,  2000,  2017,  3407,  2981,   102,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  2928,  1035, 22889, 23780,  2941,  2672,  2057,  2020,  4011,\n          2000,  3280,  1998,  2026, 13445,  5552,  2256,  3268, 27451,   102,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  1045,  2245,  1996, 19029,  1998, 14978,  2015,  2018,  2979,\n          2021,  8840,  2140,  1045,  2514,  9643,  2651,   102,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  4963, 20234,  1998, 11150,  2024,  1996,  9799,  1997,  2115,\n          7280,  2651,   102,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  2047, 13310, 23713,  2019,  8598,  2291,  2006,  2026,  2482,\n          1042,  2860,  2213,  2085,   102,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0]])\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1749677361711
        }
      },
      "id": "72ec1615"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "import random # Random module for generating random numbers and selections\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import wordnet # NLTK's WordNet corpus for finding synonyms\n",
        "\n",
        "# Define a function to find and replace a word with a synonym\n",
        "def synonym_replacement(word):\n",
        "# Get all synsets (sets of synonyms) for the given word from WordNet\n",
        "    synonyms = wordnet.synsets(word)\n",
        "\n",
        "# If the word has synonyms, randomly choose one synonym, otherwise return the original word\n",
        "    if synonyms:\n",
        "# Select a random synonym and get the first lemma (word form) of that synonym\n",
        "        return random.choice(synonyms).lemmas()[0].name()\n",
        "\n",
        "# If no synonyms are found, return the original word\n",
        "    return word\n",
        "\n",
        "# Define a function to augment text by replacing words with synonyms randomly\n",
        "def augment_text(text):\n",
        "# Split the input text into individual words\n",
        "    words = text.split() # Split the input text into individual words\n",
        "\n",
        "# Replace each word with a synonym with a probability of 20% (random.random() > 0.8)\n",
        "    augmented_words = [\n",
        "    synonym_replacement(word) if random.random() > 0.8 else word \n",
        "# If random condition met, replace\n",
        "for word in words] # Iterate over each word in the original text\n",
        "\n",
        "# Join the augmented words back into a single string and return it\n",
        "    return ' '.join(augmented_words)\n",
        "\n",
        "# Apply the text augmentation function to the 'cleaned_text' column in a DataFrame\n",
        "# Create a new column 'augmented_text' containing the augmented version of 'cleaned_text'\n",
        "data['augmented_text'] = data['cleaned_text'].apply(augment_text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package wordnet to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1749677369408
        }
      },
      "id": "d0b99a0e"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Import PyTorch library\n",
        "from torch.utils.data import TensorDataset, DataLoader # Import modules to create datasets and data loaders\n",
        "\n",
        "# Convert tokenized data into PyTorch tensors\n",
        "input_ids = tokens['input_ids'] # Extract input IDs from the tokenized data\n",
        "attention_masks = tokens['attention_mask'] # Extract attention masks from the tokenized data\n",
        "\n",
        "# Define a mapping function\n",
        "def map_sentiment(value):\n",
        "    if value == \"high\":\n",
        "        return 1\n",
        "    elif value == \"medium\":\n",
        "        return 0.5\n",
        "    elif value == \"low\":\n",
        "        return 0\n",
        "    else:\n",
        "        return None  # Handle unexpected values, if any\n",
        "\n",
        "# Apply the function to each item in 'sentiment_intensity'\n",
        "data['sentiment_intensity'] = data['sentiment_intensity'].apply(map_sentiment)\n",
        "\n",
        "# Drop any rows where 'sentiment_intensity' is None\n",
        "data = data.dropna(subset=['sentiment_intensity']).reset_index(drop=True)\n",
        "\n",
        "# Convert the 'sentiment_intensity' column to a tensor\n",
        "labels = torch.tensor(data['sentiment_intensity'].tolist())\n",
        "\n",
        "def bin_label(x):\n",
        "    if x <= 0.33:\n",
        "        return 0\n",
        "    elif x <= 0.66:\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "labels = torch.tensor([bin_label(x.item()) for x in labels])"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1749677369672
        }
      },
      "id": "5018ddfd"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_masks, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_masks = attention_masks\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_masks[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1749677369889
        }
      },
      "id": "819a4846-09cf-4439-b99e-89b0818409a9"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split # Import function to split dataset\n",
        "\n",
        "# First split: 15% for test set, the rest for training/validation\n",
        "train_val_inputs, test_inputs, train_val_masks, test_masks, train_val_labels, test_labels = train_test_split(\n",
        "    input_ids, attention_masks, labels, test_size=0.15, random_state=42\n",
        ")\n",
        "\n",
        "# Second split: 20% for validation set from remaining data\n",
        "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
        "    train_val_inputs, train_val_masks, train_val_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create TensorDataset objects for each set, including attention masks\n",
        "train_dataset =CustomDataset(train_inputs, train_masks, train_labels)\n",
        "val_dataset = CustomDataset(val_inputs, val_masks, val_labels)\n",
        "test_dataset = CustomDataset(test_inputs, test_masks, test_labels)\n",
        "\n",
        "# Create DataLoader objects\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "print(\"Training, validation, and test sets are prepared with attention masks!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Training, validation, and test sets are prepared with attention masks!\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1749677370110
        }
      },
      "id": "a84ff592"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PEFT"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "165712ca-63c6-4692-b759-8536750609f0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "# Step 1: Freeze all layers except the last one (classification head)\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# If you'd like to fine-tune additional layers (e.g., the last 2 layers), you can unfreeze those layers as well\n",
        "for param in model.base_model.encoder.layer[-2:].parameters():\n",
        "    param.requires_grad = True"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BertConfig' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertForSequenceClassification\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load config and remove problematic attribute\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mBertConfig\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m config\u001b[38;5;241m.\u001b[39mnum_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_attn_implementation\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BertConfig' is not defined"
          ]
        }
      ],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1749641200055
        }
      },
      "id": "9eb39136-f855-42a5-b5d3-ed80a19c12d7"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Step 1: Set training arguments for fine-tuning the model\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',             # Directory where results will be stored\n",
        "    learning_rate=5e-5,  # Experiment with different learning rates\n",
        "    num_train_epochs=5,                 # Number of epochs (full passes through the dataset)\n",
        "    per_device_train_batch_size=16,     # Batch size per GPU/CPU during training\n",
        "    evaluation_strategy=\"epoch\",        # Evaluate the model at the end of each epoch\n",
        ")\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": acc}\n",
        "\n",
        "# Step 2: Fine-tune only the final classification head (since earlier layers were frozen)\n",
        "trainer = Trainer(\n",
        "    model=model,                        # Pre-trained BERT model with frozen layers\n",
        "    args=training_args,                 # Training arguments\n",
        "    train_dataset=train_dataset,           # Training data for fine-tuning\n",
        "    eval_dataset=val_dataset,              # Validation data to evaluate performance during training\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Step 3: Train the model using PEFT (this performs PEFT because layers were frozen in Step 1)\n",
        "trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n/anaconda/envs/azureml_py38/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "🏃 View run ./results at: https://australiaeast.api.azureml.ms/mlflow/v2.0/subscriptions/f7f4dd05-cc7b-4921-9af7-d0cd6f686e92/resourceGroups/xyguo94-rg/providers/Microsoft.MachineLearningServices/workspaces/ai-ml-engineer/#/experiments/0f944482-fa23-4d20-985e-2864b5371056/runs/a06a56fa-e41e-4b19-8be1-5e3ef420f549\n🧪 View experiment at: https://australiaeast.api.azureml.ms/mlflow/v2.0/subscriptions/f7f4dd05-cc7b-4921-9af7-d0cd6f686e92/resourceGroups/xyguo94-rg/providers/Microsoft.MachineLearningServices/workspaces/ai-ml-engineer/#/experiments/0f944482-fa23-4d20-985e-2864b5371056\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='845' max='845' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [845/845 12:11, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.971977</td>\n      <td>0.584570</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.971196</td>\n      <td>0.584570</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.992700</td>\n      <td>0.984020</td>\n      <td>0.578635</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.992700</td>\n      <td>0.997140</td>\n      <td>0.575668</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.992700</td>\n      <td>1.004108</td>\n      <td>0.568249</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "🏃 View run ./results at: https://australiaeast.api.azureml.ms/mlflow/v2.0/subscriptions/f7f4dd05-cc7b-4921-9af7-d0cd6f686e92/resourceGroups/xyguo94-rg/providers/Microsoft.MachineLearningServices/workspaces/ai-ml-engineer/#/experiments/0f944482-fa23-4d20-985e-2864b5371056/runs/3cd0c046-f826-4827-8e4e-6072ab61450c\n🧪 View experiment at: https://australiaeast.api.azureml.ms/mlflow/v2.0/subscriptions/f7f4dd05-cc7b-4921-9af7-d0cd6f686e92/resourceGroups/xyguo94-rg/providers/Microsoft.MachineLearningServices/workspaces/ai-ml-engineer/#/experiments/0f944482-fa23-4d20-985e-2864b5371056\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 63,
          "data": {
            "text/plain": "TrainOutput(global_step=845, training_loss=0.9683402699126294, metrics={'train_runtime': 731.0016, 'train_samples_per_second': 18.413, 'train_steps_per_second': 1.156, 'total_flos': 297431218579320.0, 'train_loss': 0.9683402699126294, 'epoch': 5.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 63,
      "metadata": {
        "gather": {
          "logged": 1749640163169
        }
      },
      "id": "9fe498d0-941d-4bd7-8f2a-1d96d49b52b9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(f\"Test Accuracy: {results['eval_accuracy']}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:20]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test Accuracy: 0.5858585858585859\n"
        }
      ],
      "execution_count": 64,
      "metadata": {
        "gather": {
          "logged": 1749640184244
        }
      },
      "id": "b9b1be81-886e-4c07-8af9-45fec0491eb7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "914a16e3-0008-45bd-8576-08b7969786f1"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, AutoTokenizer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Load the pretrained model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Define LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query\", \"key\", \"value\"],  # For BERT, these are the common attention layers\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLS  # Sequence classification\n",
        ")\n",
        "\n",
        "# Wrap the model with LoRA\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# (Optional) Freeze base model weights if desired\n",
        "for name, param in model.named_parameters():\n",
        "    if \"lora\" not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Confirm which parameters are trainable\n",
        "model.print_trainable_parameters()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "trainable params: 442,368 || all params: 109,929,222 || trainable%: 0.4024\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1749642532278
        }
      },
      "id": "b2c60092-ea95-460b-b2aa-1415db6670a3"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Step 1: Set training arguments for fine-tuning the model\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',             # Directory where results will be stored\n",
        "    learning_rate=5e-5,  # Experiment with different learning rates\n",
        "    num_train_epochs=3,                 # Number of epochs (full passes through the dataset)\n",
        "    per_device_train_batch_size=16,     # Batch size per GPU/CPU during training\n",
        "    eval_strategy=\"epoch\",        # Evaluate the model at the end of each epoch\n",
        ")\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": acc}\n",
        "\n",
        "# Step 2: Fine-tune only the final classification head (since earlier layers were frozen)\n",
        "trainer = Trainer(\n",
        "    model=model,                        # Pre-trained BERT model with frozen layers\n",
        "    args=training_args,                 # Training arguments\n",
        "    train_dataset=train_dataset,           # Training data for fine-tuning\n",
        "    eval_dataset=val_dataset,              # Validation data to evaluate performance during training\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Step 3: Train the model using PEFT (this performs PEFT because layers were frozen in Step 1)\n",
        "trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "🏃 View run ./results at: https://australiaeast.api.azureml.ms/mlflow/v2.0/subscriptions/f7f4dd05-cc7b-4921-9af7-d0cd6f686e92/resourceGroups/xyguo94-rg/providers/Microsoft.MachineLearningServices/workspaces/ai-ml-engineer/#/experiments/0f944482-fa23-4d20-985e-2864b5371056/runs/f33e7a45-590a-4307-9866-74defd23ddc0\n🧪 View experiment at: https://australiaeast.api.azureml.ms/mlflow/v2.0/subscriptions/f7f4dd05-cc7b-4921-9af7-d0cd6f686e92/resourceGroups/xyguo94-rg/providers/Microsoft.MachineLearningServices/workspaces/ai-ml-engineer/#/experiments/0f944482-fa23-4d20-985e-2864b5371056\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='507' max='507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [507/507 16:19, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.982634</td>\n      <td>0.584570</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.978657</td>\n      <td>0.584570</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.010200</td>\n      <td>0.977739</td>\n      <td>0.584570</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n/anaconda/envs/azureml_py38/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "🏃 View run ./results at: https://australiaeast.api.azureml.ms/mlflow/v2.0/subscriptions/f7f4dd05-cc7b-4921-9af7-d0cd6f686e92/resourceGroups/xyguo94-rg/providers/Microsoft.MachineLearningServices/workspaces/ai-ml-engineer/#/experiments/0f944482-fa23-4d20-985e-2864b5371056/runs/6a8c5f97-cfec-480e-ac78-8c4c05f99e4b\n🧪 View experiment at: https://australiaeast.api.azureml.ms/mlflow/v2.0/subscriptions/f7f4dd05-cc7b-4921-9af7-d0cd6f686e92/resourceGroups/xyguo94-rg/providers/Microsoft.MachineLearningServices/workspaces/ai-ml-engineer/#/experiments/0f944482-fa23-4d20-985e-2864b5371056\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "TrainOutput(global_step=507, training_loss=1.0101215204543617, metrics={'train_runtime': 980.0733, 'train_samples_per_second': 8.24, 'train_steps_per_second': 0.517, 'total_flos': 179385259534992.0, 'train_loss': 1.0101215204543617, 'epoch': 3.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1749643867758
        }
      },
      "id": "7cf9a942-48f8-40c6-b622-afd4ef03f2af"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QLoRA"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "0b68d83d-5483-40e7-ba41-692c80f65bce"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2ForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "\n",
        "# Enable 8-bit quantization via bitsandbytes\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,  # QLoRA: load model in 8-bit\n",
        "    llm_int8_threshold=6.0,\n",
        "    llm_int8_skip_modules=None,\n",
        "    load_in_8bit_fp32_cpu_offload=True\n",
        ")\n",
        "\n",
        "# Load GPT-2 model for sequence classification in 8-bit\n",
        "quantized_model = GPT2ForSequenceClassification.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    num_labels=3,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Define LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    target_modules=[\"c_attn\"]  # GPT-2 uses fused attention projection layer\n",
        ")\n",
        "\n",
        "# Apply LoRA to quantized model\n",
        "quantized_model = get_peft_model(quantized_model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "quantized_model.print_trainable_parameters()\n",
        "\n",
        "# (Optional) Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT2 doesn't have a pad token by default\n",
        "\n",
        "for name, param in quantized_model.named_parameters():\n",
        "    if \"lora\" not in name:\n",
        "        param.requires_grad = False"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\nUnused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "No GPU found. A GPU is needed for quantization.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      7\u001b[0m     load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# QLoRA: load model in 8-bit\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     llm_int8_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6.0\u001b[39m,\n\u001b[1;32m      9\u001b[0m     llm_int8_skip_modules\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     load_in_8bit_fp32_cpu_offload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load GPT-2 model for sequence classification in 8-bit\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2ForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Define LoRA config\u001b[39;00m\n\u001b[1;32m     22\u001b[0m lora_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     23\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     24\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# GPT-2 uses fused attention projection layer\u001b[39;00m\n\u001b[1;32m     29\u001b[0m )\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/transformers/modeling_utils.py:3202\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3199\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3202\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3205\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3206\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:62\u001b[0m, in \u001b[0;36mBnb8BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     66\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1749677374496
        }
      },
      "id": "a63ff710-2eb8-4805-afa3-dea6b85c1352"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    evaluation_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "# Fine-tune the QLoRA-enhanced model\n",
        "trainer = Trainer(\n",
        "    model=quantized_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "5bfa047f-ed07-4da5-8780-ae34f76f9cc4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(f\"Test Accuracy: {results['eval_accuracy']}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "8ea8fafa-15d9-4962-8e8a-80113a49063c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.10 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}