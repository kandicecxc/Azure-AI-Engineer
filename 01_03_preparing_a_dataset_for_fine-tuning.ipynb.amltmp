{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install modules\n",
        "# A '!' in a Jupyter Notebook runs the line in the system's shell, and not in the Python interpreter\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load dataset \n",
        "# you can download this dataset from https://huggingface.co/datasets/stepp1/tweet_emotion_intensity/tree/main\n",
        "data = pd.read_csv('data/tweet_emotion_intensity/train.csv')\n",
        "\n",
        "# Preview the data\n",
        "print(data.head())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1749123564284
        }
      },
      "id": "1c38d690"
    },
    {
      "cell_type": "code",
      "source": [
        "import re # Import the `re` module for working with regular expressions\n",
        "\n",
        "# Function to clean the text\n",
        "def clean_text(text):\n",
        "    text = text.lower() # Convert all text to lowercase for uniformity\n",
        "    text = re.sub(r'http\\S+', '', text) # Remove URLs from the text\n",
        "    text = re.sub(r'<.*?>', '', text) # Remove any HTML tags from the text\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation, keep only words and spaces\n",
        "    return text # Return the cleaned text\n",
        "\n",
        "# Assume `data` is a pandas DataFrame with a column named 'text'\n",
        "# Apply the cleaning function to each row of the 'text' column\n",
        "data['cleaned_text'] = data['tweet'].apply(clean_text)\n",
        "\n",
        "# Print the first 5 rows of the cleaned text to verify the cleaning process\n",
        "print(data['cleaned_text'].head())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1749123564341
        }
      },
      "id": "26033b2c"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the dataset\n",
        "print(data.isnull().sum()) # Print the count of missing values for each column\n",
        "\n",
        "# Option 1: Remove rows with missing data in the 'cleaned_text' column\n",
        "data = data.dropna(subset=['cleaned_text']) # Drop rows where 'cleaned_text' is NaN (missing)\n",
        "\n",
        "# Option 2: Fill missing values in 'cleaned_text' with a placeholder\n",
        "data['cleaned_text'].fillna('unknown', inplace=True) # Replace NaN values in 'cleaned_text' with 'unknown'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1749123564353
        }
      },
      "id": "5093335d"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the cleaned text\n",
        "tokens = tokenizer(\n",
        "    data['cleaned_text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt'\n",
        ")\n",
        "\n",
        "print(tokens['input_ids'][:5])  # Preview the first 5 tokenized examples"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'transformers'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the BERT tokenizer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1749123565313
        }
      },
      "id": "72ec1615"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "import random # Random module for generating random numbers and selections\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import wordnet # NLTK's WordNet corpus for finding synonyms\n",
        "\n",
        "# Define a function to find and replace a word with a synonym\n",
        "def synonym_replacement(word):\n",
        "# Get all synsets (sets of synonyms) for the given word from WordNet\n",
        "    synonyms = wordnet.synsets(word)\n",
        "\n",
        "# If the word has synonyms, randomly choose one synonym, otherwise return the original word\n",
        "    if synonyms:\n",
        "# Select a random synonym and get the first lemma (word form) of that synonym\n",
        "        return random.choice(synonyms).lemmas()[0].name()\n",
        "\n",
        "# If no synonyms are found, return the original word\n",
        "    return word\n",
        "\n",
        "# Define a function to augment text by replacing words with synonyms randomly\n",
        "def augment_text(text):\n",
        "# Split the input text into individual words\n",
        "    words = text.split() # Split the input text into individual words\n",
        "\n",
        "# Replace each word with a synonym with a probability of 20% (random.random() > 0.8)\n",
        "    augmented_words = [\n",
        "    synonym_replacement(word) if random.random() > 0.8 else word \n",
        "# If random condition met, replace\n",
        "for word in words] # Iterate over each word in the original text\n",
        "\n",
        "# Join the augmented words back into a single string and return it\n",
        "    return ' '.join(augmented_words)\n",
        "\n",
        "# Apply the text augmentation function to the 'cleaned_text' column in a DataFrame\n",
        "# Create a new column 'augmented_text' containing the augmented version of 'cleaned_text'\n",
        "data['augmented_text'] = data['cleaned_text'].apply(augment_text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\kandicec\\AppData\\Roaming\\nltk_data...\n"
        }
      ],
      "execution_count": 10,
      "metadata": {},
      "id": "d0b99a0e"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Import PyTorch library\n",
        "from torch.utils.data import TensorDataset, DataLoader # Import modules to create datasets and data loaders\n",
        "\n",
        "# Convert tokenized data into PyTorch tensors\n",
        "input_ids = tokens['input_ids'] # Extract input IDs from the tokenized data\n",
        "attention_masks = tokens['attention_mask'] # Extract attention masks from the tokenized data\n",
        "\n",
        "# Define a mapping function\n",
        "def map_sentiment(value):\n",
        "    if value == \"high\":\n",
        "        return 1\n",
        "    elif value == \"medium\":\n",
        "        return 0.5\n",
        "    elif value == \"low\":\n",
        "        return 0\n",
        "    else:\n",
        "        return None  # Handle unexpected values, if any\n",
        "\n",
        "# Apply the function to each item in 'sentiment_intensity'\n",
        "data['sentiment_intensity'] = data['sentiment_intensity'].apply(map_sentiment)\n",
        "\n",
        "# Drop any rows where 'sentiment_intensity' is None\n",
        "data = data.dropna(subset=['sentiment_intensity']).reset_index(drop=True)\n",
        "\n",
        "# Convert the 'sentiment_intensity' column to a tensor\n",
        "labels = torch.tensor(data['sentiment_intensity'].tolist())"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {},
      "id": "5018ddfd"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split # Import function to split dataset\n",
        "\n",
        "# First split: 15% for test set, the rest for training/validation\n",
        "train_val_inputs, test_inputs, train_val_masks, test_masks, train_val_labels, test_labels = train_test_split(\n",
        "    input_ids, attention_masks, labels, test_size=0.15, random_state=42\n",
        ")\n",
        "\n",
        "# Second split: 20% for validation set from remaining data\n",
        "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
        "    train_val_inputs, train_val_masks, train_val_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create TensorDataset objects for each set, including attention masks\n",
        "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "\n",
        "# Create DataLoader objects\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "print(\"Training, validation, and test sets are prepared with attention masks!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Training, validation, and test sets are prepared with attention masks!\n"
        }
      ],
      "execution_count": 12,
      "metadata": {},
      "id": "a84ff592"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.10 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}