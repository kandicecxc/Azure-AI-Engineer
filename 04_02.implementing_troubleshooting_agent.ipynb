{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic data\n",
        "n_samples = 1000\n",
        "data = {\n",
        "    'timestamp': pd.date_range(start='2024-01-01', periods=n_samples, freq='h'),\n",
        "    'cpu_usage': np.random.normal(50, 10, n_samples),       # CPU usage in percentage\n",
        "    'memory_usage': np.random.normal(60, 15, n_samples),    # Memory usage in percentage\n",
        "    'network_latency': np.random.normal(100, 20, n_samples), # Network latency in ms\n",
        "    'disk_io': np.random.normal(75, 10, n_samples),         # Disk I/O in MB/s\n",
        "    'error_rate': np.random.choice([0, 1], n_samples, p=[0.95, 0.05])  # 5% error rate\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n",
        "print(df.info())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "            timestamp  cpu_usage  memory_usage  network_latency    disk_io  \\\n0 2024-01-01 00:00:00  54.967142     80.990332        86.496435  55.921924   \n1 2024-01-01 01:00:00  48.617357     73.869505        97.109627  66.396150   \n2 2024-01-01 02:00:00  56.476885     60.894456        84.151602  70.863945   \n3 2024-01-01 03:00:00  65.230299     50.295948        93.840769  93.876877   \n4 2024-01-01 04:00:00  47.658466     70.473350        62.127707  80.565531   \n\n   error_rate  \n0           0  \n1           0  \n2           1  \n3           0  \n4           0  \n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 6 columns):\n #   Column           Non-Null Count  Dtype         \n---  ------           --------------  -----         \n 0   timestamp        1000 non-null   datetime64[ns]\n 1   cpu_usage        1000 non-null   float64       \n 2   memory_usage     1000 non-null   float64       \n 3   network_latency  1000 non-null   float64       \n 4   disk_io          1000 non-null   float64       \n 5   error_rate       1000 non-null   int64         \ndtypes: datetime64[ns](1), float64(4), int64(1)\nmemory usage: 47.0 KB\nNone\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1749717060301
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Implement anomaly detection using Isolation Forest\n",
        "def detect_anomalies(data):\n",
        "    model = IsolationForest(contamination=0.05, random_state=42)\n",
        "    model.fit(data)\n",
        "    anomalies = model.predict(data)\n",
        "    return anomalies\n",
        "\n",
        "# Detect anomalies in the dataset \n",
        "numeric_data = df.select_dtypes(include=[float, int]) # Only numeric columns \n",
        "df['anomaly'] = detect_anomalies(numeric_data)\n",
        "\n",
        "print(df['anomaly'].value_counts()) # -1 denotes an anomaly"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": " 1    950\n-1     50\nName: anomaly, dtype: int64\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1749717071401
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import zscore\n",
        "\n",
        "# Calculate z-scores to identify anomalous values per column in anomalous rows\n",
        "z_scores = numeric_data.apply(zscore)\n",
        "\n",
        "# Function to identify anomalous columns for each row\n",
        "def find_anomalous_columns(row, threshold=3):\n",
        "    return [col for col in numeric_data.columns if abs(z_scores.loc[row.name, col]) > threshold]\n",
        "\n",
        "# Apply the function to each anomalous row\n",
        "df['anomalous_columns'] = df.apply(lambda row: find_anomalous_columns(row) if row['anomaly'] == -1 else [], axis=1)\n",
        "\n",
        "# Display rows with anomalies and their anomalous columns\n",
        "print(df[df['anomaly'] == -1][['timestamp', 'anomaly', 'anomalous_columns']])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "              timestamp  anomaly              anomalous_columns\n37  2024-01-02 13:00:00       -1                   [error_rate]\n38  2024-01-02 14:00:00       -1                   [error_rate]\n62  2024-01-03 14:00:00       -1                   [error_rate]\n132 2024-01-06 12:00:00       -1                   [error_rate]\n179 2024-01-08 11:00:00       -1                   [error_rate]\n192 2024-01-09 00:00:00       -1                   [error_rate]\n208 2024-01-09 16:00:00       -1                   [error_rate]\n241 2024-01-11 01:00:00       -1                   [error_rate]\n245 2024-01-11 05:00:00       -1                   [error_rate]\n251 2024-01-11 11:00:00       -1                   [error_rate]\n262 2024-01-11 22:00:00       -1        [cpu_usage, error_rate]\n272 2024-01-12 08:00:00       -1                   [error_rate]\n285 2024-01-12 21:00:00       -1                   [error_rate]\n315 2024-01-14 03:00:00       -1                   [error_rate]\n329 2024-01-14 17:00:00       -1                   [error_rate]\n330 2024-01-14 18:00:00       -1                   [error_rate]\n334 2024-01-14 22:00:00       -1                   [error_rate]\n350 2024-01-15 14:00:00       -1                   [error_rate]\n354 2024-01-15 18:00:00       -1                   [error_rate]\n371 2024-01-16 11:00:00       -1                   [error_rate]\n386 2024-01-17 02:00:00       -1                   [error_rate]\n387 2024-01-17 03:00:00       -1                   [error_rate]\n413 2024-01-18 05:00:00       -1                   [error_rate]\n471 2024-01-20 15:00:00       -1                             []\n489 2024-01-21 09:00:00       -1                   [error_rate]\n498 2024-01-21 18:00:00       -1                   [error_rate]\n505 2024-01-22 01:00:00       -1                   [error_rate]\n521 2024-01-22 17:00:00       -1  [network_latency, error_rate]\n544 2024-01-23 16:00:00       -1                             []\n586 2024-01-25 10:00:00       -1                   [error_rate]\n602 2024-01-26 02:00:00       -1                   [error_rate]\n626 2024-01-27 02:00:00       -1                   [error_rate]\n639 2024-01-27 15:00:00       -1                   [error_rate]\n671 2024-01-28 23:00:00       -1                   [error_rate]\n675 2024-01-29 03:00:00       -1                   [error_rate]\n676 2024-01-29 04:00:00       -1                   [error_rate]\n716 2024-01-30 20:00:00       -1          [disk_io, error_rate]\n720 2024-01-31 00:00:00       -1                   [error_rate]\n735 2024-01-31 15:00:00       -1                   [error_rate]\n739 2024-01-31 19:00:00       -1                   [error_rate]\n758 2024-02-01 14:00:00       -1                   [error_rate]\n779 2024-02-02 11:00:00       -1                   [error_rate]\n826 2024-02-04 10:00:00       -1                   [error_rate]\n844 2024-02-05 04:00:00       -1                   [error_rate]\n861 2024-02-05 21:00:00       -1                   [error_rate]\n872 2024-02-06 08:00:00       -1                   [error_rate]\n903 2024-02-07 15:00:00       -1                   [error_rate]\n910 2024-02-07 22:00:00       -1                   [error_rate]\n933 2024-02-08 21:00:00       -1                   [error_rate]\n940 2024-02-09 04:00:00       -1                   [error_rate]\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1749717084570
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Train a decision tree for root cause analysis\n",
        "def root_cause_analysis(X_train, y_train, X_test):\n",
        "    model = DecisionTreeClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    return predictions\n",
        "\n",
        "# Example root cause analysis (assuming data is preprocessed)\n",
        "X_train = df.drop(['anomaly', 'anomalous_columns'] + df.select_dtypes(include=['datetime64']).columns.tolist(), axis=1)\n",
        "y_train = df['anomaly']\n",
        "predicted_causes = root_cause_analysis(X_train, y_train, X_train)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1749717396865
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example solution recommendation based on root cause\n",
        "def recommend_solution(root_cause):\n",
        "    solutions = {\n",
        "        \"network_error\": \"Restart the network service.\",\n",
        "        \"database_issue\": \"Check the database connection and restart the service.\",\n",
        "        \"high_cpu_usage\": \"Optimize running processes or allocate more resources.\"\n",
        "    }\n",
        "    return solutions.get(root_cause, \"No recommendation available.\")\n",
        "\n",
        "# Recommend a solution based on a detected root cause\n",
        "solution = recommend_solution(\"network_error\")\n",
        "print(f\"Recommended solution: {solution}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Recommended solution: Restart the network service.\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1749717405128
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate a network error by altering the dataset\n",
        "df.loc[0, 'network_latency'] = 1000  # Simulating high network latency\n",
        "\n",
        "# Run the troubleshooting agent\n",
        "anomalies = detect_anomalies(df.select_dtypes(include=[float, int]))\n",
        "predicted_causes = root_cause_analysis(X_train, y_train, df.drop(columns=['anomalous_columns', 'anomaly', 'timestamp']))\n",
        "solution = recommend_solution(predicted_causes[0])\n",
        "print(f\"Detected issue: {predicted_causes[0]}\")\n",
        "print(f\"Recommended solution: {solution}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Detected issue: 1\nRecommended solution: No recommendation available.\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1749717542033
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.10 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}