{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Sample incoming data\n",
        "incoming_data = pd.DataFrame({'feature1': [1.4, 1.6, 1.8], 'feature2': [3.3, 3.9, 4.2]})\n",
        "\n",
        "# Training data metrics\n",
        "training_mean = {'feature1': 1.5, 'feature2': 3.7}\n",
        "training_std = {'feature1': 0.2, 'feature2': 0.3}\n",
        "\n",
        "# Calculate statistics for incoming data\n",
        "incoming_mean = incoming_data.mean()\n",
        "\n",
        "# Compare data to check for drift\n",
        "for feature in incoming_data.columns:\n",
        "    if abs(incoming_mean[feature] - training_mean[feature]) > training_std[feature] * 3:\n",
        "        print(f\"Alert: Significant data deviation detected in {feature}\")"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1749899981904
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example Prometheus configuration to scrape model metrics\n",
        "# scrape_configs:\n",
        "#   - job_name: 'model_metrics'\n",
        "#     static_configs:\n",
        "#       - targets: ['localhost:9090']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.monitor.query import MetricsQueryClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "client = MetricsQueryClient(credential)\n",
        "\n",
        "# Alert example\n",
        "alert_condition = {\n",
        "    \"threshold\": 85,\n",
        "    \"metric\": \"accuracy\",\n",
        "    \"operator\": \"LessThan\",\n",
        "    \"alert_action\": \"EmailNotification\"\n",
        "}\n",
        "\n",
        "print(\"Alert condition set for accuracy below 85%.\")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'azure.monitor'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmonitor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetricsQueryClient\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential\n\u001b[1;32m      4\u001b[0m credential \u001b[38;5;241m=\u001b[39m DefaultAzureCredential()\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'azure.monitor'"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1749900008615
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ks_2samp\n",
        "\n",
        "# Training data and incoming data samples\n",
        "training_data_sample = np.random.normal(1.5, 0.2, 100)\n",
        "incoming_data_sample = incoming_data['feature1']\n",
        "\n",
        "# Perform KS test\n",
        "d_stat, p_value = ks_2samp(training_data_sample, incoming_data_sample)\n",
        "if p_value < 0.05:\n",
        "    print(\"Model drift detected for feature1\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# Configure logging settings\n",
        "logging.basicConfig(filename='model_performance_logs.log', level=logging.INFO)\n",
        "\n",
        "# Log prediction request and response\n",
        "input_data = {'feature1': 1.6, 'feature2': 3.8}\n",
        "output_prediction = {'prediction': 0.9}\n",
        "\n",
        "logging.info(f\"Input: {input_data}, Output: {output_prediction}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}